{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf0ac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import random_split\n",
    "from collections import Counter\n",
    "\n",
    "torch.manual_seed(123)\n",
    "torch.set_default_dtype(torch.double) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5e049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar(train_val_split=0.9, data_path='../data/', preprocessor=None):\n",
    "    \n",
    "    # Define preprocessor if not already given\n",
    "    if preprocessor is None:\n",
    "        preprocessor = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                                (0.2470, 0.2435, 0.2616))\n",
    "        ])\n",
    "    \n",
    "    # load datasets\n",
    "    data_train_val = datasets.CIFAR10(\n",
    "        data_path,       \n",
    "        train=True,      \n",
    "        download=True,  \n",
    "        transform=preprocessor)\n",
    "\n",
    "    data_test = datasets.CIFAR10(\n",
    "        data_path, \n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=preprocessor)\n",
    "\n",
    "    # train/validation split\n",
    "    n_train = int(len(data_train_val)*train_val_split)\n",
    "    n_val =  len(data_train_val) - n_train\n",
    "\n",
    "    data_train, data_val = random_split(\n",
    "        data_train_val, \n",
    "        [n_train, n_val],\n",
    "        generator=torch.Generator().manual_seed(123)\n",
    "    )\n",
    "\n",
    "    print(\"Size of the train dataset:        \", len(data_train))\n",
    "    print(\"Size of the validation dataset:   \", len(data_val))\n",
    "    print(\"Size of the test dataset:         \", len(data_test))\n",
    "    \n",
    "    return (data_train, data_val, data_test)\n",
    "\n",
    "cifar10_train, cifar10_val, cifar10_test = load_cifar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e87c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#label_map is a dictionary where the keys are original class labels, and the values are the new class labels.\n",
    "#it's mapping the original class label 0 to the new label 0 and the original class label 2 to the new label 1. \n",
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "\n",
    "# For each part of dataset, keep only airplanes and birds. it means that we create new datasets \n",
    "#(cifar2_train, cifar2_val, and cifar2_test) by filtering the original datasets (cifar10_train, cifar10_val, \n",
    "# and cifar10_test) to only include the selected classes (0 and 2) and mapping their labels using label_map.\n",
    "#training data set:\n",
    "cifar2_train = [(img, label_map[label]) for img, label in cifar10_train if label in [0, 2]]\n",
    "#validation: \n",
    "cifar2_val = [(img, label_map[label]) for img, label in cifar10_val if label in [0, 2]]\n",
    "#test set:\n",
    "cifar2_test = [(img, label_map[label]) for img, label in cifar10_test if label in [0, 2]]\n",
    "\n",
    "print('Size of the training dataset: ', len(cifar2_train))\n",
    "print('Size of the validation dataset: ', len(cifar2_val))\n",
    "print('Size of the test dataset: ', len(cifar2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd173448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network\n",
    "\n",
    "class MyMLP(nn.Module):\n",
    "    \n",
    "    #Architecture\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(32*32*3, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32,30)\n",
    "    \n",
    "    #Forward Pass:  \n",
    "    def forward(self, x): \n",
    "        out = torch.flatten(x, 1) \n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = F.relu(self.fc3(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2922412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n",
    "def train(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    \n",
    "    n_batch = len(train_loader)\n",
    "    \n",
    "    # We'll store there the training loss for each epoch\n",
    "    losses_train = []\n",
    "    \n",
    "    # Set the network in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Re-initialize gradients, just in case the model has been inappropriately \n",
    "    # manipulated before the training\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1): \n",
    "        \n",
    "        # Training loss for the current epoch\n",
    "        loss_train = 0\n",
    "\n",
    "        # Loop over our dataset (in batches the data loader creates for us)\n",
    "        for imgs, labels in train_loader:\n",
    "                     \n",
    "            # Feed a batch into our model\n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            # Compute the loss we wish to minimize \n",
    "            # Note that by default, it is the mean loss that is computed\n",
    "            # (so entire_batch_loss / batch_size)\n",
    "            loss = loss_fn(outputs, labels) \n",
    "            \n",
    "            # Perform the backward step. That is, compute the gradients of all parameters we want the network to learn\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the model\n",
    "            optimizer.step() \n",
    "            \n",
    "            # Zero out gradients before the next round (or the end of training)\n",
    "            optimizer.zero_grad() \n",
    "\n",
    "            # Update loss for this epoch\n",
    "            # It is important to transform the loss to a number with .item()\n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        # Store current epoch loss. \n",
    "        losses_train.append(loss_train / n_batch)\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.3f}'.format(\n",
    "                datetime.now().time(), epoch, loss_train / n_batch))\n",
    "            \n",
    "    return losses_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e75752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop manually update\n",
    "\n",
    "def train_manual_update(n_epochs, lr, model, loss_fn, train_loader):\n",
    "    \n",
    "    n_batch = len(train_loader)\n",
    "    losses_train = []\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1): \n",
    "        \n",
    "        loss_train = 0\n",
    "\n",
    "        for imgs, labels in train_loader:\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for par in model.parameters():\n",
    "                    par = (par @ model.fc4[0](lr)) * par.grad()\n",
    "                    \n",
    "\n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        losses_train.append(loss_train / n_batch)\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.3f}'.format(\n",
    "                datetime.now().time(), epoch, loss_train / n_batch))\n",
    "            \n",
    "    return losses_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be752f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with train_manual_update\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2_train, batch_size=64, shuffle=False) \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "torch.manual_seed(123)\n",
    "model = MyMLP()\n",
    "\n",
    "manual_update_loss = train_manual_update(\n",
    "    n_epochs = 21,\n",
    "    lr = 0.05,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "print(manual_update_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca1973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with train()-function\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2_train, batch_size=64, shuffle=False) \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "torch.manual_seed(123)\n",
    "model = MyMLP()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05, momentum=0)\n",
    "\n",
    "losssss = train(\n",
    "    n_epochs = 21,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "print(losssss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13079d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified train_manual_update (L2 regularization / ridge)\n",
    "\n",
    "def train_manual_update2(n_epochs, lr, model, loss_fn, train_loader):\n",
    "    \n",
    "    n_batch = len(train_loader)\n",
    "    losses_train = []\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1): \n",
    "        \n",
    "        loss_train = 0\n",
    "\n",
    "        for imgs, labels in train_loader:\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for par in model.parameters():\n",
    "                    par -= lr * par.grad\n",
    "                    \n",
    "            model.zero_grad()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        losses_train.append(loss_train / n_batch)\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.3f}'.format(\n",
    "                datetime.now().time(), epoch, loss_train / n_batch))\n",
    "            \n",
    "    return losses_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3aa1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(123)\n",
    "model = MyMLP()\n",
    "train_loader = torch.utils.data.DataLoader(cifar2_train, batch_size=64, shuffle=False) \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "manual_update_lossss = train_manual_update2(\n",
    "    n_epochs = 21,\n",
    "    lr = 0.05,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    weight_decay = 0.5\n",
    ")\n",
    "print(manual_update_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec54de2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f951a94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
