{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "31d674fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Magnus\n",
      "[nltk_data]     Lohne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from datetime import datetime\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d2c1d6",
   "metadata": {},
   "source": [
    "# Task 2.1 - Word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca09dae",
   "metadata": {},
   "source": [
    "## Task 2.1.1 - Tokenizing\n",
    "We start by defining a function that takes a file path, reads the file and returns the text. Then, we tokenize it using the nltk package. The we define the lists of txt files, and run them through the function to define the lists of tokens. We use the functions defined in the tutorial for handling text data and adjust them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fb3fa435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer will split a long text into a list of english words\n",
    "TOKENIZER_EN = get_tokenizer('basic_english')\n",
    "# Where we will store / load all our models, datasets, vocabulary, etc.\n",
    "PATH_GENERATED = '../Project 3/'\n",
    "# Minimum number of occurence of a word in the text to add it to the vocabulary\n",
    "MIN_FREQ = 100\n",
    "\n",
    "def read_files(datapath='./data_train/'):\n",
    "    \"\"\"\n",
    "    Return a list of strings, one for each line in each .txt files in 'datapath'\n",
    "    \"\"\"\n",
    "    # Find all txt files in directory \n",
    "    files = os.listdir(datapath)\n",
    "    files = [datapath + f for f in files if f.endswith(\".txt\")]\n",
    "    \n",
    "    # Stores each line of each book in a list\n",
    "    lines = []\n",
    "    for f_name in files:\n",
    "        with open(f_name, encoding=\"utf8\") as f:\n",
    "            lines += f.readlines()\n",
    "    return lines\n",
    "\n",
    "def tokenize(lines, tokenizer=TOKENIZER_EN):\n",
    "    \"\"\"\n",
    "    Tokenize the list of lines\n",
    "    \"\"\"\n",
    "    list_text = []\n",
    "    for line in lines:\n",
    "        list_text += tokenizer(line)\n",
    "    return list_text\n",
    "\n",
    "def yield_tokens(lines, tokenizer=TOKENIZER_EN):\n",
    "    \"\"\"\n",
    "    Yield tokens, ignoring names and digits to build vocabulary\n",
    "    \"\"\"\n",
    "    # Match any word containing digit\n",
    "    no_digits = '\\w*[0-9]+\\w*'\n",
    "    # Match word containing a uppercase \n",
    "    no_names = '\\w*[A-Z]+\\w*'\n",
    "    # Match any sequence containing more than one space\n",
    "    no_spaces = '\\s+'\n",
    "    \n",
    "    for line in lines:\n",
    "        line = re.sub(no_digits, ' ', line)\n",
    "        line = re.sub(no_names, ' ', line)\n",
    "        line = re.sub(no_spaces, ' ', line)\n",
    "        yield tokenizer(line)\n",
    "        \n",
    "# ----------------------- Tokenize texts -------------------------------\n",
    "# Load tokenized versions of texts if you have already generated it\n",
    "# Otherwise, create it and save it\n",
    "if os.path.isfile(PATH_GENERATED + \"words_train.pt\"):\n",
    "    words_train = torch.load(PATH_GENERATED + \"words_train.pt\")\n",
    "    words_val = torch.load(PATH_GENERATED + \"words_val.pt\")\n",
    "    words_test = torch.load(PATH_GENERATED + \"words_test.pt\")\n",
    "else:\n",
    "    # Get lists of strings, one for each line in each .txt files in 'datapath' \n",
    "    lines_books_train = read_files('./data_train/')\n",
    "    lines_books_val = read_files('./data_val/')\n",
    "    lines_books_test = read_files('./data_test/')\n",
    "\n",
    "    # List of words contained in the dataset\n",
    "    words_train = tokenize(lines_books_train)\n",
    "    words_val = tokenize(lines_books_val)\n",
    "    words_test = tokenize(lines_books_test)\n",
    "    \n",
    "    torch.save(words_train , PATH_GENERATED + \"words_train.pt\")\n",
    "    torch.save(words_val , PATH_GENERATED + \"words_val.pt\")\n",
    "    torch.save(words_test , PATH_GENERATED + \"words_test.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fee4bd",
   "metadata": {},
   "source": [
    "## Task 2.1.2 - Vocabulary\n",
    "We define a vocabulary and print some information about it. We use the functions defined in the tutorial for handling text data and adjust them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8d12647e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the training dataset:      2684706\n",
      "Total number of words in the validation dataset:    49526\n",
      "Total number of words in the test dataset:          124152\n",
      "Number of distinct words in the training dataset:   52105\n",
      "Number of distinct words kept (vocabulary size):    1880\n"
     ]
    }
   ],
   "source": [
    "# ----------------------- Create vocabulary ----------------------------\n",
    "\n",
    "def count_freqs(words, vocab):\n",
    "    \"\"\"\n",
    "    Count occurrences of each word in vocabulary in the data\n",
    "    \n",
    "    Useful to get some insight on the data and to compute loss weights\n",
    "    \"\"\"\n",
    "    freqs = torch.zeros(len(vocab), dtype=torch.int)\n",
    "    for w in words:\n",
    "        freqs[vocab[w]] += 1\n",
    "    return freqs\n",
    "\n",
    "def create_vocabulary(lines, min_freq=MIN_FREQ):\n",
    "    \"\"\"\n",
    "    Create a vocabulary (list of known tokens) from a list of strings\n",
    "    \"\"\"\n",
    "    # vocab contains the vocabulary found in the data, associating an index to each word\n",
    "    vocab = build_vocab_from_iterator(yield_tokens(lines), min_freq=min_freq, specials=[\"<unk>\"])\n",
    "    # Since we removed all words with an uppercase when building the vocabulary, we skipped the word \"I\"\n",
    "    vocab.append_token(\"i\")\n",
    "    # Value of default index. This index will be returned when OOV (Out Of Vocabulary) token is queried.\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    return vocab\n",
    "\n",
    "VOCAB_FNAME = \"vocabulary.pt\"\n",
    "# Load vocabulary if you have already generated it\n",
    "# Otherwise, create it and save it\n",
    "if os.path.isfile(PATH_GENERATED + VOCAB_FNAME):\n",
    "    vocab = torch.load(PATH_GENERATED + VOCAB_FNAME)\n",
    "else:\n",
    "    # Create vocabulary based on the words in the training dataset\n",
    "    vocab = create_vocabulary(lines_books_train, min_freq=MIN_FREQ)\n",
    "    torch.save(vocab, PATH_GENERATED + VOCAB_FNAME)\n",
    "    \n",
    "\n",
    "\n",
    "# ------------------------ Quick analysis ------------------------------\n",
    "VOCAB_SIZE = len(vocab)\n",
    "print(\"Total number of words in the training dataset:     \", len(words_train))\n",
    "print(\"Total number of words in the validation dataset:   \", len(words_val))\n",
    "print(\"Total number of words in the test dataset:         \", len(words_test))\n",
    "print(\"Number of distinct words in the training dataset:  \", len(set(words_train)))\n",
    "print(\"Number of distinct words kept (vocabulary size):   \", VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd500176",
   "metadata": {},
   "source": [
    "Given the size of the training dataset, the vocabulary is quite small. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab7c8c",
   "metadata": {},
   "source": [
    "## Task 2.1.3 - CBOW Model\n",
    "We start by defining the training dataset with context and target words. Then we define a simple architecture. We use the functions defined in the tutorial for handling text data and adjust them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "034eb74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ Define targets ------------------------------\n",
    "def compute_label(w):\n",
    "    \"\"\"\n",
    "    helper function to define MAP_TARGET\n",
    "    \n",
    "    - 0 = 'unknown word'\n",
    "    - 1 = 'punctuation' (i.e. the '<unk>' token)\n",
    "    - 2 = 'is an actual word'\n",
    "    \"\"\"\n",
    "    if w in ['<unk>']:\n",
    "        return 0\n",
    "    elif w in [',', '.', '(', ')', '?', '!']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "# true labels for this task:\n",
    "MAP_TARGET = {\n",
    "    vocab[w]:compute_label(w) for w in vocab.lookup_tokens(range(VOCAB_SIZE))\n",
    "}\n",
    "\n",
    "# context size for this task \n",
    "CONTEXT_SIZE = 3\n",
    "\n",
    "\n",
    "# ---------------- Define context / target pairs -----------------------\n",
    "def create_dataset(\n",
    "    text, vocab, \n",
    "    context_size=CONTEXT_SIZE, map_target=MAP_TARGET\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a pytorch dataset of context / target pairs from a text\n",
    "    \"\"\"\n",
    "    \n",
    "    n_text = len(text)\n",
    "    n_vocab = len(vocab)\n",
    "    \n",
    "    # Change labels if only a few target are kept, otherwise, each word is\n",
    "    # associated with its index in the vocabulary\n",
    "    if map_target is None:\n",
    "        map_target = {i:i for i in range(n_vocab)}\n",
    "    \n",
    "    # Transform the text as a list of integers.\n",
    "    txt = [vocab[w] for w in text]\n",
    "\n",
    "    # Start constructing the context / target pairs...\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    for i in range(n_text - context_size):\n",
    "        \n",
    "        # Word used to define target\n",
    "        t = txt[i + context_size]\n",
    "        \n",
    "        # Context before the target\n",
    "        c = txt[i:i + context_size]\n",
    "        \n",
    "        targets.append(map_target[t])\n",
    "        contexts.append(torch.tensor(c))\n",
    "            \n",
    "    # contexts of shape (N_dataset, context_size)\n",
    "    # targets of shape  (N_dataset)\n",
    "    contexts = torch.stack(contexts)\n",
    "    targets = torch.tensor(targets)\n",
    "    # Create a pytorch dataset out of these context / target pairs\n",
    "    return TensorDataset(contexts, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "93804a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(words, vocab, fname):\n",
    "    \"\"\"\n",
    "    Load dataset if already generated, otherwise, create it and save it\n",
    "    \"\"\"\n",
    "    # If already generated\n",
    "    if os.path.isfile(PATH_GENERATED + fname):\n",
    "        dataset = torch.load(PATH_GENERATED + fname)\n",
    "    else:\n",
    "        # Create context / target dataset based on the list of strings\n",
    "        dataset = create_dataset(words, vocab)\n",
    "        torch.save(dataset, PATH_GENERATED + fname)\n",
    "    return dataset\n",
    "\n",
    "data_train = load_dataset(words_train, vocab, \"data_train.pt\")\n",
    "data_val = load_dataset(words_val, vocab, \"data_val.pt\")\n",
    "data_test = load_dataset(words_test, vocab, \"data_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e776ac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, context_idxs):\n",
    "        embeds = self.embeddings(context_idxs)\n",
    "        out = torch.mean(embeds, dim=1)\n",
    "        log_probs = self.linear(out)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feca1d79",
   "metadata": {},
   "source": [
    "## Task 2.1.4 - Training several models\n",
    "We define a training loop and a function to compute accuracy. Then we train models with the architecture above with different hyperparameters, i.e. embedding dimension, batch size and learning rate. Then we choose the best model, based on validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "cf04eb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optimizer, data_loader, n_epochs):\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        for context_idxs, target_idxs in data_loader:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            log_probs = model(context_idxs)\n",
    "            \n",
    "            loss = loss_fn(log_probs, target_idxs)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print('{}  |  Epoch {}  |  Training loss {:.3f}'.format(\n",
    "            datetime.now().time(), epoch+1, total_loss / len(data_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "0a59306a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for contexts, targets in loader:\n",
    "\n",
    "            outputs = model(contexts)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += len(targets)\n",
    "            correct += int((predicted == targets).sum())\n",
    "\n",
    "    acc =  correct / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "80bda639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/21, Loss: 0.7568479743020895\n",
      "Epoch 2/21, Loss: 0.7391480775697865\n",
      "Epoch 3/21, Loss: 0.7386429603932441\n",
      "Epoch 4/21, Loss: 0.7383203918089883\n",
      "Epoch 5/21, Loss: 0.7380729230083924\n",
      "Epoch 6/21, Loss: 0.7379986406026211\n",
      "Epoch 7/21, Loss: 0.7380109585937072\n",
      "Epoch 8/21, Loss: 0.7379625817561463\n",
      "Epoch 9/21, Loss: 0.7379501215919977\n",
      "Epoch 10/21, Loss: 0.7380440340650238\n",
      "Epoch 11/21, Loss: 0.7380130876346765\n",
      "Epoch 12/21, Loss: 0.7380772951959285\n",
      "Epoch 13/21, Loss: 0.7381345853078067\n",
      "Epoch 14/21, Loss: 0.7381831162450424\n",
      "Epoch 15/21, Loss: 0.7382165207467858\n",
      "Epoch 16/21, Loss: 0.7382367236196498\n",
      "Epoch 17/21, Loss: 0.7382709037177753\n",
      "Epoch 18/21, Loss: 0.7383780183183991\n",
      "Epoch 19/21, Loss: 0.7382899303913685\n",
      "Epoch 20/21, Loss: 0.7383282051296711\n",
      "Epoch 21/21, Loss: 0.7382786875544061\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(13)\n",
    "\n",
    "vocab_size = VOCAB_SIZE\n",
    "embedding_dim = 10\n",
    "\n",
    "cbow_model1 = CBOWModel(vocab_size, embedding_dim)\n",
    "optimizer = optim.Adam(cbow_model1.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=128, shuffle=True)\n",
    "\n",
    "train(\n",
    "    model = cbow_model1, \n",
    "    loss_fn = loss_fn, \n",
    "    optimizer = optimizer,\n",
    "    data_loader = train_loader, \n",
    "    n_epochs = 21\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a68a669e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:     0.7129\n",
      "Validation Accuracy:   0.7303\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(data_val, batch_size=128, shuffle=False)\n",
    "\n",
    "acc_train = compute_accuracy(cbow_model1, train_loader)\n",
    "acc_val = compute_accuracy(cbow_model1, val_loader)\n",
    "print(\"Training Accuracy:     %.4f\" %acc_train)\n",
    "print(\"Validation Accuracy:   %.4f\" %acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c688f76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/21, Loss: 0.8596063743046839\n",
      "Epoch 2/21, Loss: 0.7370489141341472\n",
      "Epoch 3/21, Loss: 0.7358415521086328\n",
      "Epoch 4/21, Loss: 0.7356351911183335\n",
      "Epoch 5/21, Loss: 0.7355914771400561\n",
      "Epoch 6/21, Loss: 0.7355422079378431\n",
      "Epoch 7/21, Loss: 0.7355288976707959\n",
      "Epoch 8/21, Loss: 0.73546467530429\n",
      "Epoch 9/21, Loss: 0.7354956221182668\n",
      "Epoch 10/21, Loss: 0.7354541537147881\n",
      "Epoch 11/21, Loss: 0.7354502648420641\n",
      "Epoch 12/21, Loss: 0.7354375941435117\n",
      "Epoch 13/21, Loss: 0.735459428430314\n",
      "Epoch 14/21, Loss: 0.7354125745418671\n",
      "Epoch 15/21, Loss: 0.7354130342208444\n",
      "Epoch 16/21, Loss: 0.7354271591320651\n",
      "Epoch 17/21, Loss: 0.7354087704270901\n",
      "Epoch 18/21, Loss: 0.7354136936215593\n",
      "Epoch 19/21, Loss: 0.7353675464197051\n",
      "Epoch 20/21, Loss: 0.7353538356378622\n",
      "Epoch 21/21, Loss: 0.7353685641601344\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(13)\n",
    "\n",
    "vocab_size = VOCAB_SIZE\n",
    "embedding_dim = 12\n",
    "\n",
    "cbow_model2 = CBOWModel(vocab_size, embedding_dim)\n",
    "optimizer = optim.Adam(cbow_model2.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=128, shuffle=True)\n",
    "\n",
    "train(\n",
    "    model = cbow_model2, \n",
    "    loss_fn = loss_fn, \n",
    "    optimizer = optimizer,\n",
    "    data_loader = train_loader, \n",
    "    n_epochs = 21\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "807afda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:     0.0490\n",
      "Validation Accuracy:   0.1214\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(data_val, batch_size=128, shuffle=False)\n",
    "\n",
    "acc_train = compute_accuracy(cbow_model2, train_loader)\n",
    "acc_val2 = compute_accuracy(cbow_model2, val_loader)\n",
    "print(\"Training Accuracy:     %.4f\" %acc_train)\n",
    "print(\"Validation Accuracy:   %.4f\" %acc_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "90bc9e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/21, Loss: 1.0898744947100123\n",
      "Epoch 2/21, Loss: 0.7428367885694896\n",
      "Epoch 3/21, Loss: 0.736730985064983\n",
      "Epoch 4/21, Loss: 0.7355983536789928\n",
      "Epoch 5/21, Loss: 0.7353192499855108\n",
      "Epoch 6/21, Loss: 0.7352139426496173\n",
      "Epoch 7/21, Loss: 0.7351928712369649\n",
      "Epoch 8/21, Loss: 0.7351608302395913\n",
      "Epoch 9/21, Loss: 0.7351475127615336\n",
      "Epoch 10/21, Loss: 0.7351378691755661\n",
      "Epoch 11/21, Loss: 0.7351301855980941\n",
      "Epoch 12/21, Loss: 0.7351212146785039\n",
      "Epoch 13/21, Loss: 0.7351147522233443\n",
      "Epoch 14/21, Loss: 0.7351035327060271\n",
      "Epoch 15/21, Loss: 0.7351067304202021\n",
      "Epoch 16/21, Loss: 0.7351044692172976\n",
      "Epoch 17/21, Loss: 0.7351011715937352\n",
      "Epoch 18/21, Loss: 0.7350970284116805\n",
      "Epoch 19/21, Loss: 0.7350812663816478\n",
      "Epoch 20/21, Loss: 0.7350749991760283\n",
      "Epoch 21/21, Loss: 0.7350564083918463\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(13)\n",
    "\n",
    "vocab_size = VOCAB_SIZE\n",
    "embedding_dim = 16\n",
    "\n",
    "cbow_model3 = CBOWModel(vocab_size, embedding_dim)\n",
    "optimizer = optim.Adam(cbow_model3.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=512, shuffle=True)\n",
    "\n",
    "train(\n",
    "    model = cbow_model3, \n",
    "    loss_fn = loss_fn, \n",
    "    optimizer = optimizer,\n",
    "    data_loader = train_loader, \n",
    "    n_epochs = 21\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "de32cfdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:     0.7138\n",
      "Validation Accuracy:   0.7307\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(data_val, batch_size=128, shuffle=False)\n",
    "\n",
    "acc_train = compute_accuracy(cbow_model3, train_loader)\n",
    "acc_val3 = compute_accuracy(cbow_model3, val_loader)\n",
    "print(\"Training Accuracy:     %.4f\" %acc_train)\n",
    "print(\"Validation Accuracy:   %.4f\" %acc_val3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878e09ab",
   "metadata": {},
   "source": [
    "Choosing the best model, using a model selection function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f1577f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 had the highest validation accuracy\n",
      "\n",
      "Validation accuracy: 0.7308725238777941\n"
     ]
    }
   ],
   "source": [
    "val_accs = [acc_val, acc_val2, acc_val3]\n",
    "\n",
    "def model_selection(val_accs):\n",
    "    return f\"Model {np.argmax(val_accs)+1} had the highest validation accuracy\\n\\nValidation accuracy: {max(val_accs)}\"\n",
    "\n",
    "print(model_selection(val_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "3c669745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 0.2562485400607335\n"
     ]
    }
   ],
   "source": [
    "test_loader = torch.utils.data.DataLoader(data_test, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f'Test accuracy = {compute_accuracy(cbow_model2, test_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9687074",
   "metadata": {},
   "source": [
    "## Task 2.1.5 - Cosine similarity\n",
    "We compute the cosine similarity matrix for the vocabulary, given a the embedding above. Then we print the most similar words to some example words and their similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "d0528efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_embeddings = cbow_model2.embeddings.weight.data\n",
    "normed_embeddings = F.normalize(trained_embeddings, p=2, dim=1)\n",
    "cosine_similarity_matrix = torch.mm(normed_embeddings, normed_embeddings.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "de8184ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to 'me':\n",
      "yourself (similarity: 0.9330)\n",
      "thee (similarity: 0.9247)\n",
      "myself (similarity: 0.8752)\n",
      "indeed (similarity: 0.8520)\n",
      "mine (similarity: 0.8328)\n",
      "us (similarity: 0.8162)\n",
      "yours (similarity: 0.7865)\n",
      "i (similarity: 0.7827)\n",
      "certainly (similarity: 0.7681)\n",
      "done (similarity: 0.7424)\n",
      "\n",
      "\n",
      "Most similar words to 'white':\n",
      "red (similarity: 0.9831)\n",
      "blue (similarity: 0.9700)\n",
      "heavy (similarity: 0.9562)\n",
      "deep (similarity: 0.9554)\n",
      "black (similarity: 0.9486)\n",
      "sharp (similarity: 0.9402)\n",
      "large (similarity: 0.9353)\n",
      "soft (similarity: 0.9331)\n",
      "yellow (similarity: 0.9319)\n",
      "thick (similarity: 0.9156)\n",
      "\n",
      "\n",
      "Most similar words to 'man':\n",
      "woman (similarity: 0.9433)\n",
      "person (similarity: 0.9169)\n",
      "dogs (similarity: 0.9166)\n",
      "soldier (similarity: 0.9162)\n",
      "creature (similarity: 0.9106)\n",
      "lady (similarity: 0.9105)\n",
      "boy (similarity: 0.8961)\n",
      "servant (similarity: 0.8941)\n",
      "maid (similarity: 0.8936)\n",
      "husband (similarity: 0.8936)\n",
      "\n",
      "\n",
      "Most similar words to 'have':\n",
      "has (similarity: 0.9597)\n",
      "having (similarity: 0.9417)\n",
      "had (similarity: 0.8900)\n",
      "offered (similarity: 0.7796)\n",
      "trying (similarity: 0.7188)\n",
      "hast (similarity: 0.7163)\n",
      "absolutely (similarity: 0.6860)\n",
      "managed (similarity: 0.6625)\n",
      "rule (similarity: 0.6333)\n",
      "be (similarity: 0.6328)\n",
      "\n",
      "\n",
      "Most similar words to 'be':\n",
      "being (similarity: 0.9237)\n",
      "been (similarity: 0.8976)\n",
      "are (similarity: 0.8711)\n",
      "is (similarity: 0.8147)\n",
      "becomes (similarity: 0.8089)\n",
      "was (similarity: 0.7580)\n",
      "were (similarity: 0.7020)\n",
      "re (similarity: 0.6992)\n",
      "am (similarity: 0.6941)\n",
      "became (similarity: 0.6861)\n",
      "\n",
      "\n",
      "Most similar words to 'child':\n",
      "doctor (similarity: 0.9608)\n",
      "lady (similarity: 0.9536)\n",
      "boy (similarity: 0.9508)\n",
      "woman (similarity: 0.9453)\n",
      "girl (similarity: 0.9420)\n",
      "stranger (similarity: 0.9374)\n",
      "gentleman (similarity: 0.9373)\n",
      "porter (similarity: 0.9327)\n",
      "husband (similarity: 0.9312)\n",
      "priest (similarity: 0.9229)\n",
      "\n",
      "\n",
      "Most similar words to 'yes':\n",
      "sir (similarity: 0.8487)\n",
      "why (similarity: 0.7793)\n",
      "excellency (similarity: 0.7525)\n",
      "here (similarity: 0.7263)\n",
      "however (similarity: 0.7150)\n",
      "gentlemen (similarity: 0.7027)\n",
      "indeed (similarity: 0.6891)\n",
      "mine (similarity: 0.6841)\n",
      "too (similarity: 0.6764)\n",
      "minute (similarity: 0.6721)\n",
      "\n",
      "\n",
      "Most similar words to 'what':\n",
      "how (similarity: 0.8716)\n",
      "why (similarity: 0.7996)\n",
      "whatever (similarity: 0.6719)\n",
      "whom (similarity: 0.6523)\n",
      "if (similarity: 0.6322)\n",
      "here (similarity: 0.6231)\n",
      "that (similarity: 0.6113)\n",
      "art (similarity: 0.5952)\n",
      "anything (similarity: 0.5939)\n",
      "thinking (similarity: 0.5867)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {word: index for index, word in enumerate(vocab.get_itos())}\n",
    "index_to_word = {index: word for word, index in word_to_index.items()}\n",
    "\n",
    "def k_most_similar_words(word, vocab, k):\n",
    "    word_idx = word_to_index[word]\n",
    "    sim_scores = cosine_similarity_matrix[word_idx]\n",
    "    top_scores, top_indices = torch.topk(sim_scores, k+1)\n",
    "    \n",
    "    similar_words = [(index_to_word[idx.item()], score.item()) for idx, score in zip(top_indices[1:], top_scores[1:])]\n",
    "    \n",
    "    result = f\"Most similar words to '{word}':\\n\"\n",
    "    result += \"\\n\".join([f\"{word_sim[0]} (similarity: {word_sim[1]:.4f})\" for word_sim in similar_words])\n",
    "    return result + \"\\n\\n\"\n",
    "\n",
    "words = [\"me\", \"white\", \"man\", \"have\", \"be\", \"child\", \"yes\", \"what\"]\n",
    "\n",
    "for word in words:\n",
    "    print(k_most_similar_words(word, vocab, k=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03610b53",
   "metadata": {},
   "source": [
    "We can see that the ten most similar words actually are quite similar to the example words, at least a great part of them. With the word 'yes', there were mostly similar words that did not make sense, however.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f017de",
   "metadata": {},
   "source": [
    "# Task 2.2 - Conjugating *be* and *have*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9bb4f1",
   "metadata": {},
   "source": [
    "## Task 2.2.1 - Defining RNN and MLP architecture\n",
    "First of all, we need to define a dataset where the targets are the particular words *be, am, are, is, was, were, been, being, have, has, had, having*. After this, we define a simple MLP first, then the RNN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "c4477a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_be_have_dataset(text, vocab, context_size=3, specific_targets=None):\n",
    "\n",
    "    target_to_index = {target: i for i, target in enumerate(specific_targets)}\n",
    "    n_text = len(text)\n",
    "    txt = [vocab[w] for w in text]\n",
    "    contexts = []\n",
    "    targets = [] \n",
    "    for i in range(n_text - context_size):\n",
    "        target_word = text[i + context_size]\n",
    "        \n",
    "        if target_word not in specific_targets:\n",
    "            continue\n",
    "        \n",
    "        target_index = target_to_index[target_word]\n",
    "        context = txt[i:i + context_size]\n",
    "\n",
    "        targets.append(target_index)\n",
    "        contexts.append(torch.tensor(context, dtype=torch.long))\n",
    "\n",
    "    contexts = torch.stack(contexts)\n",
    "    targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "    return TensorDataset(contexts, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "d8091bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_be_have_dataset(words, vocab, fname):\n",
    "\n",
    "    # If already generated\n",
    "    if os.path.isfile(PATH_GENERATED + fname):\n",
    "        dataset = torch.load(PATH_GENERATED + fname)\n",
    "    else:\n",
    "        # Create context / target dataset based on the list of strings\n",
    "        targets = [\"be\", \"am\", \"are\", \"is\", \"was\", \"were\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\"]\n",
    "        dataset = create_be_have_dataset(words, vocab, specific_targets=targets)\n",
    "        torch.save(dataset, PATH_GENERATED + fname)\n",
    "    return dataset\n",
    "\n",
    "data_train_conjugate = load_be_have_dataset(words_train, vocab, \"data_train_conjugation.pt\")\n",
    "data_val_conjugate = load_be_have_dataset(words_val, vocab, \"data_val_conjugation.pt\")\n",
    "data_test_conjugate = load_be_have_dataset(words_test, vocab, \"data_test_conjugation.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "7fd27b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, pretrained_embeddings, hidden_size, num_classes):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n",
    "        input_features = 3 * 12 \n",
    "        self.fc1 = nn.Linear(input_features, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "f5b407a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConjugationRNN(nn.Module):\n",
    "    def __init__(self, pretrained_embeddings, hidden_size, num_classes):\n",
    "        super(ConjugationRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n",
    "        self.lstm = nn.LSTM(pretrained_embeddings.size(1), hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        x = lstm_out[:, -1, :]\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a67f187",
   "metadata": {},
   "source": [
    "## Task 2.2.2 - Training several models\n",
    "We train different models and then compute their training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "8751e4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:54:07.539031  |  Epoch 1  |  Training loss 1.495\n",
      "14:54:10.423324  |  Epoch 2  |  Training loss 1.339\n",
      "14:54:12.628337  |  Epoch 3  |  Training loss 1.307\n",
      "14:54:14.820748  |  Epoch 4  |  Training loss 1.294\n",
      "14:54:17.058169  |  Epoch 5  |  Training loss 1.288\n",
      "14:54:19.244533  |  Epoch 6  |  Training loss 1.280\n",
      "14:54:21.788528  |  Epoch 7  |  Training loss 1.276\n",
      "14:54:24.151099  |  Epoch 8  |  Training loss 1.271\n",
      "14:54:26.598739  |  Epoch 9  |  Training loss 1.266\n",
      "14:54:28.777532  |  Epoch 10  |  Training loss 1.264\n",
      "14:54:30.976935  |  Epoch 11  |  Training loss 1.260\n",
      "14:54:33.146028  |  Epoch 12  |  Training loss 1.258\n",
      "14:54:35.657199  |  Epoch 13  |  Training loss 1.254\n",
      "14:54:37.913783  |  Epoch 14  |  Training loss 1.252\n",
      "14:54:40.653162  |  Epoch 15  |  Training loss 1.250\n",
      "14:54:42.998909  |  Epoch 16  |  Training loss 1.248\n",
      "14:54:45.240181  |  Epoch 17  |  Training loss 1.246\n",
      "14:54:47.379212  |  Epoch 18  |  Training loss 1.245\n",
      "14:54:49.629362  |  Epoch 19  |  Training loss 1.243\n",
      "14:54:52.187023  |  Epoch 20  |  Training loss 1.242\n",
      "14:54:54.485314  |  Epoch 21  |  Training loss 1.240\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(13)\n",
    "\n",
    "simple_mlp1 = SimpleMLP(pretrained_embeddings = trained_embeddings, \n",
    "                        hidden_size = 10, num_classes = 12)\n",
    "\n",
    "optimizer = optim.Adam(simple_mlp1.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "train_loader = torch.utils.data.DataLoader(data_train_conjugate, batch_size=512, shuffle=True)\n",
    "\n",
    "train(\n",
    "    model = simple_mlp1, \n",
    "    loss_fn = loss_fn, \n",
    "    optimizer = optimizer,\n",
    "    data_loader = train_loader, \n",
    "    n_epochs = 21\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "cefaa717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:     0.4637\n",
      "Validation Accuracy:   0.4579\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(data_val_conjugate, batch_size=128, shuffle=False)\n",
    "\n",
    "acc_train = compute_accuracy(simple_mlp1, train_loader)\n",
    "acc_val = compute_accuracy(simple_mlp1, val_loader)\n",
    "print(\"Training Accuracy:     %.4f\" %acc_train)\n",
    "print(\"Validation Accuracy:   %.4f\" %acc_val3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "fdff8832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:27.127544  |  Epoch 1  |  Training loss 1.527\n",
      "14:56:29.796404  |  Epoch 2  |  Training loss 1.324\n",
      "14:56:33.053674  |  Epoch 3  |  Training loss 1.298\n",
      "14:56:35.795347  |  Epoch 4  |  Training loss 1.285\n",
      "14:56:38.511040  |  Epoch 5  |  Training loss 1.275\n",
      "14:56:41.515107  |  Epoch 6  |  Training loss 1.269\n",
      "14:56:44.246470  |  Epoch 7  |  Training loss 1.263\n",
      "14:56:46.896097  |  Epoch 8  |  Training loss 1.257\n",
      "14:56:49.827285  |  Epoch 9  |  Training loss 1.256\n",
      "14:56:52.551694  |  Epoch 10  |  Training loss 1.250\n",
      "14:56:55.573496  |  Epoch 11  |  Training loss 1.248\n",
      "14:56:58.244525  |  Epoch 12  |  Training loss 1.246\n",
      "14:57:00.884368  |  Epoch 13  |  Training loss 1.242\n",
      "14:57:03.978466  |  Epoch 14  |  Training loss 1.240\n",
      "14:57:06.893126  |  Epoch 15  |  Training loss 1.238\n",
      "14:57:09.557850  |  Epoch 16  |  Training loss 1.234\n",
      "14:57:12.272973  |  Epoch 17  |  Training loss 1.233\n",
      "14:57:14.960042  |  Epoch 18  |  Training loss 1.231\n",
      "14:57:18.111286  |  Epoch 19  |  Training loss 1.230\n",
      "14:57:21.087484  |  Epoch 20  |  Training loss 1.227\n",
      "14:57:24.318700  |  Epoch 21  |  Training loss 1.224\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(13)\n",
    "\n",
    "trained_embeddings = cbow_model2.embeddings.weight.data\n",
    "\n",
    "conjugation_rnn = ConjugationRNN(pretrained_embeddings = trained_embeddings,\n",
    "                                 hidden_size = 10, num_classes = 12)\n",
    "\n",
    "optimizer = optim.Adam(conjugation_rnn.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "train_loader = torch.utils.data.DataLoader(data_train_conjugate, batch_size=512, shuffle=True)\n",
    "\n",
    "train(\n",
    "    model = conjugation_rnn, \n",
    "    loss_fn = loss_fn, \n",
    "    optimizer = optimizer,\n",
    "    data_loader = train_loader, \n",
    "    n_epochs = 21\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "5d929200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:     0.4713\n",
      "Validation Accuracy:   0.4579\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(data_val_conjugate, batch_size=128, shuffle=False)\n",
    "\n",
    "acc_train = compute_accuracy(conjugation_rnn, train_loader)\n",
    "acc_val2 = compute_accuracy(conjugation_rnn, val_loader)\n",
    "print(\"Training Accuracy:     %.4f\" %acc_train)\n",
    "print(\"Validation Accuracy:   %.4f\" %acc_val3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "d14d2527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:58:02.133004  |  Epoch 1  |  Training loss 1.433\n",
      "14:58:04.985147  |  Epoch 2  |  Training loss 1.272\n",
      "14:58:08.255441  |  Epoch 3  |  Training loss 1.255\n",
      "14:58:11.518217  |  Epoch 4  |  Training loss 1.245\n",
      "14:58:14.280389  |  Epoch 5  |  Training loss 1.237\n",
      "14:58:17.397564  |  Epoch 6  |  Training loss 1.231\n",
      "14:58:20.266093  |  Epoch 7  |  Training loss 1.228\n",
      "14:58:23.141125  |  Epoch 8  |  Training loss 1.222\n",
      "14:58:26.120047  |  Epoch 9  |  Training loss 1.218\n",
      "14:58:28.765073  |  Epoch 10  |  Training loss 1.215\n",
      "14:58:31.792394  |  Epoch 11  |  Training loss 1.212\n",
      "14:58:34.415940  |  Epoch 12  |  Training loss 1.209\n",
      "14:58:37.080469  |  Epoch 13  |  Training loss 1.207\n",
      "14:58:40.227737  |  Epoch 14  |  Training loss 1.204\n",
      "14:58:43.022375  |  Epoch 15  |  Training loss 1.202\n",
      "14:58:45.710324  |  Epoch 16  |  Training loss 1.200\n",
      "14:58:48.347113  |  Epoch 17  |  Training loss 1.197\n",
      "14:58:51.025813  |  Epoch 18  |  Training loss 1.196\n",
      "14:58:53.976760  |  Epoch 19  |  Training loss 1.194\n",
      "14:58:56.777445  |  Epoch 20  |  Training loss 1.191\n",
      "14:58:59.868495  |  Epoch 21  |  Training loss 1.190\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(13)\n",
    "\n",
    "trained_embeddings = cbow_model2.embeddings.weight.data\n",
    "conjugation_rnn2 = ConjugationRNN(pretrained_embeddings = trained_embeddings, \n",
    "                                  hidden_size = 16, num_classes = 12)\n",
    "\n",
    "optimizer = optim.Adam(conjugation_rnn2.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "train_loader = torch.utils.data.DataLoader(data_train_conjugate, batch_size=512, shuffle=True)\n",
    "\n",
    "train(\n",
    "    model = conjugation_rnn2, \n",
    "    loss_fn = loss_fn, \n",
    "    optimizer = optimizer,\n",
    "    data_loader = train_loader, \n",
    "    n_epochs = 21\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "2a3530cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:     0.5211\n",
      "Validation Accuracy:   0.4579\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(data_val_conjugate, batch_size=128, shuffle=False)\n",
    "\n",
    "acc_train = compute_accuracy(conjugation_rnn2, train_loader)\n",
    "acc_val3 = compute_accuracy(conjugation_rnn2, val_loader)\n",
    "print(\"Training Accuracy:     %.4f\" %acc_train)\n",
    "print(\"Validation Accuracy:   %.4f\" %acc_val3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "e598bd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3 had the highest validation accuracy\n",
      "\n",
      "Validation accuracy: 0.45791505791505793\n"
     ]
    }
   ],
   "source": [
    "val_accs = [acc_val, acc_val2, acc_val3]\n",
    "print(model_selection(val_accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515f32cc",
   "metadata": {},
   "source": [
    "We can see that the RNN architectures performed a little better than the simple MLP in terms of accuracy. They also use approximately the same time training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "0935e565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 0.40650577124868836\n"
     ]
    }
   ],
   "source": [
    "test_loader = torch.utils.data.DataLoader(data_test_conjugate, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f'Test accuracy = {compute_accuracy(conjugation_rnn2, test_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346c1464",
   "metadata": {},
   "source": [
    "# Task 2.3 - Text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadef9a7",
   "metadata": {},
   "source": [
    "## Task 2.3.1 - Predicting the next word\n",
    "We define an RNN architecture that based on a context can predict the next word of the sequence. Before this, however, we need to create a new dataset for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "c5851422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_generation_dataset(text, vocab, context_size=2):\n",
    "\n",
    "    n_text = len(text)\n",
    "    txt = [vocab[w] for w in text] \n",
    "\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    for i in range(context_size, n_text):\n",
    "        context = txt[i-context_size:i] \n",
    "        target = txt[i] \n",
    "        contexts.append(torch.tensor(context, dtype=torch.long))\n",
    "        targets.append(target)\n",
    "\n",
    "    contexts = torch.stack(contexts)\n",
    "    targets = torch.tensor(targets, dtype=torch.long)\n",
    "    return TensorDataset(contexts, targets)\n",
    "\n",
    "data_train_generate = create_text_generation_dataset(words_train, vocab, CONTEXT_SIZE)\n",
    "data_val_generate = create_text_generation_dataset(words_val, vocab, CONTEXT_SIZE)\n",
    "data_test_generate = create_text_generation_dataset(words_test, vocab, CONTEXT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "9f7d6f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNPredicting(nn.Module):\n",
    "    def __init__(self, pretrained_embeddings, hidden_size, output_size):\n",
    "        super(RNNPredicting, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n",
    "        embedding_dim = pretrained_embeddings.size(1)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embedding(x)\n",
    "        x, hidden = self.rnn(x, hidden)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22760515",
   "metadata": {},
   "source": [
    "## Task 2.3.2 - Training several models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "11d27a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:56:02.959698  |  Epoch 1  |  Training loss 4.385\n",
      "15:58:52.781815  |  Epoch 2  |  Training loss 4.273\n",
      "16:01:34.001148  |  Epoch 3  |  Training loss 4.242\n",
      "16:04:15.600314  |  Epoch 4  |  Training loss 4.226\n",
      "16:07:04.520755  |  Epoch 5  |  Training loss 4.214\n",
      "16:09:55.754391  |  Epoch 6  |  Training loss 4.209\n",
      "16:12:45.212371  |  Epoch 7  |  Training loss 4.205\n",
      "16:15:29.895858  |  Epoch 8  |  Training loss 4.201\n",
      "16:18:16.003278  |  Epoch 9  |  Training loss 4.197\n",
      "16:20:54.136697  |  Epoch 10  |  Training loss 4.194\n",
      "16:23:31.764411  |  Epoch 11  |  Training loss 4.192\n",
      "16:26:12.557297  |  Epoch 12  |  Training loss 4.190\n",
      "16:28:53.708228  |  Epoch 13  |  Training loss 4.188\n",
      "16:31:34.710182  |  Epoch 14  |  Training loss 4.187\n",
      "16:34:24.042504  |  Epoch 15  |  Training loss 4.185\n",
      "16:37:25.692957  |  Epoch 16  |  Training loss 4.183\n",
      "16:40:26.423674  |  Epoch 17  |  Training loss 4.180\n",
      "16:43:27.517089  |  Epoch 18  |  Training loss 4.178\n",
      "16:46:13.274395  |  Epoch 19  |  Training loss 4.176\n",
      "16:48:54.407384  |  Epoch 20  |  Training loss 4.174\n",
      "16:51:40.201659  |  Epoch 21  |  Training loss 4.171\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(13)\n",
    "\n",
    "trained_embeddings = cbow_model2.embeddings.weight.data\n",
    "\n",
    "predicting_rnn = RNNPredicting(pretrained_embeddings = trained_embeddings, \n",
    "                               hidden_size = 16, output_size = VOCAB_SIZE)\n",
    "\n",
    "optimizer = optim.Adam(predicting_rnn.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "train_loader = torch.utils.data.DataLoader(data_train_generate, batch_size=512, shuffle=True)\n",
    "\n",
    "train(\n",
    "    model = predicting_rnn, \n",
    "    loss_fn = loss_fn, \n",
    "    optimizer = optimizer,\n",
    "    data_loader = train_loader, \n",
    "    n_epochs = 21\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "91e2517f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:     0.1980\n",
      "Validation Accuracy:   0.1929\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(data_val_generate, batch_size=128, shuffle=False)\n",
    "\n",
    "acc_train = compute_accuracy(predicting_rnn, train_loader)\n",
    "acc_val = compute_accuracy(predicting_rnn, val_loader)\n",
    "print(\"Training Accuracy:     %.4f\" %acc_train)\n",
    "print(\"Validation Accuracy:   %.4f\" %acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "324f908e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:04:33.715233  |  Epoch 1  |  Training loss 4.158\n",
      "17:07:31.976969  |  Epoch 2  |  Training loss 4.082\n",
      "17:10:31.303135  |  Epoch 3  |  Training loss 4.066\n",
      "18:21:14.832738  |  Epoch 4  |  Training loss 4.059\n",
      "18:23:46.185132  |  Epoch 5  |  Training loss 4.054\n",
      "18:26:25.773285  |  Epoch 6  |  Training loss 4.050\n",
      "18:29:04.194332  |  Epoch 7  |  Training loss 4.047\n",
      "18:32:04.116195  |  Epoch 8  |  Training loss 4.045\n",
      "18:34:51.720145  |  Epoch 9  |  Training loss 4.044\n",
      "18:37:36.027472  |  Epoch 10  |  Training loss 4.042\n",
      "18:40:27.191111  |  Epoch 11  |  Training loss 4.041\n",
      "18:43:06.258644  |  Epoch 12  |  Training loss 4.040\n",
      "18:45:37.763255  |  Epoch 13  |  Training loss 4.039\n",
      "18:48:08.862609  |  Epoch 14  |  Training loss 4.038\n",
      "18:50:41.502874  |  Epoch 15  |  Training loss 4.038\n",
      "18:53:14.726007  |  Epoch 16  |  Training loss 4.037\n",
      "18:56:02.570742  |  Epoch 17  |  Training loss 4.037\n",
      "18:58:45.260631  |  Epoch 18  |  Training loss 4.036\n",
      "19:01:29.817532  |  Epoch 19  |  Training loss 4.036\n",
      "19:04:03.262133  |  Epoch 20  |  Training loss 4.036\n",
      "19:06:37.740744  |  Epoch 21  |  Training loss 4.036\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(13)\n",
    "\n",
    "trained_embeddings = cbow_model2.embeddings.weight.data\n",
    "\n",
    "predicting_rnn2 = RNNPredicting(pretrained_embeddings = trained_embeddings, \n",
    "                               hidden_size = 32, output_size = VOCAB_SIZE)\n",
    "\n",
    "optimizer = optim.Adam(predicting_rnn2.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "train_loader = torch.utils.data.DataLoader(data_train_generate, batch_size=512, shuffle=True)\n",
    "\n",
    "train(\n",
    "    model = predicting_rnn2, \n",
    "    loss_fn = loss_fn, \n",
    "    optimizer = optimizer,\n",
    "    data_loader = train_loader, \n",
    "    n_epochs = 21\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "16ffd224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:     0.2322\n",
      "Validation Accuracy:   0.2182\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(data_val_generate, batch_size=128, shuffle=False)\n",
    "\n",
    "acc_train = compute_accuracy(predicting_rnn2, train_loader)\n",
    "acc_val2 = compute_accuracy(predicting_rnn2, val_loader)\n",
    "print(\"Training Accuracy:     %.4f\" %acc_train)\n",
    "print(\"Validation Accuracy:   %.4f\" %acc_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "68df45c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 had the highest validation accuracy\n",
      "\n",
      "Validation accuracy: 0.21824202895624256\n"
     ]
    }
   ],
   "source": [
    "val_accs = [acc_val, acc_val2]\n",
    "print(model_selection(val_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "7353b1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 0.262756848625442\n"
     ]
    }
   ],
   "source": [
    "test_loader = torch.utils.data.DataLoader(data_test_generate, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f'Test accuracy = {compute_accuracy(predicting_rnn2, test_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3693dbd",
   "metadata": {},
   "source": [
    "## Task 2.3.3 - Beam search algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "e300faac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(model, initial_context, vocab, beam_width=3, max_len=10):\n",
    "\n",
    "    model.eval()\n",
    "    initial_context = torch.tensor(initial_context, dtype=torch.long).unsqueeze(0)\n",
    "    sequences = [(initial_context, 0)]\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        all_candidates = []\n",
    "        for seq, score in sequences:\n",
    "            with torch.no_grad():\n",
    "                output = model(seq)\n",
    "                probabilities = F.softmax(output[-1], dim=0)\n",
    "                probabilities[vocab['<unk>']] = 0\n",
    "\n",
    "            top_probs, top_indices = torch.topk(probabilities, beam_width)\n",
    "            for i in range(beam_width):\n",
    "                next_word_idx = top_indices[i].unsqueeze(0)\n",
    "                next_word_log_prob = top_probs[i].item()\n",
    "                new_seq = torch.cat((seq, next_word_idx.unsqueeze(0)), dim=1)\n",
    "                new_score = score + next_word_log_prob\n",
    "                all_candidates.append((new_seq, new_score))\n",
    "\n",
    "        all_candidates.sort(key=lambda tup: tup[1], reverse=True)\n",
    "        sequences = all_candidates[:beam_width]\n",
    "\n",
    "    best_sequence = sequences[0][0]\n",
    "    best_sequence = best_sequence.squeeze().tolist()\n",
    "    generated_words = [vocab.lookup_token(idx) if idx < len(vocab) else \"<unk>\" for idx in best_sequence]\n",
    "\n",
    "    return generated_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da0edc4",
   "metadata": {},
   "source": [
    "## Task 2.1.4 - Playing with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "b3bea2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def playing_with_beam(phrases, max_len, beam_width):\n",
    "    for phrase in range(len(phrases)):\n",
    "        phrase_context = [vocab.get_stoi()[word] for word in phrases[phrase].split()][:CONTEXT_SIZE]\n",
    "        generated_text = beam_search(predicting_rnn2, phrase_context, vocab, beam_width=beam_width, max_len=max_len)\n",
    "        print(f\"Starting with phrase '{phrases[phrase]}': {' '.join(generated_text)}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "63986b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with phrase 'i want': i want to be done ,\n",
      "\n",
      "\n",
      "Starting with phrase 'he needed': he needed , and i have\n",
      "\n",
      "\n",
      "Starting with phrase 'the man': the man , and i have\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phrases = [\"i want\", \"he needed\", \"the man\"]\n",
    "playing_with_beam(phrases, max_len=4, beam_width=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "0c4838c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with phrase 'i want': i want to be able to be seen , and i have\n",
      "\n",
      "\n",
      "Starting with phrase 'he needed': he needed , and i have no reason . it is a\n",
      "\n",
      "\n",
      "Starting with phrase 'the man': the man s son , said the other , and i have\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phrases = [\"i want\", \"he needed\", \"the man\"]\n",
    "playing_with_beam(phrases, max_len=10, beam_width=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "eac5a6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with phrase 'i want': i want to be able to be able to be able to\n",
      "\n",
      "\n",
      "Starting with phrase 'he needed': he needed , and it was not to be done , and\n",
      "\n",
      "\n",
      "Starting with phrase 'the man': the man in the middle of the night , and it was\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phrases = [\"i want\", \"he needed\", \"the man\"]\n",
    "playing_with_beam(phrases, max_len=10, beam_width=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119538e0",
   "metadata": {},
   "source": [
    "Some of the sentences made okay sense, maybe better with a higher beam width. After this last cell, the model just generated the same words over and over. Thus, we decided to remove it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
